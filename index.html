<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/html">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DIME">
  <meta name="keywords" content="Diffusion-Based Reinforcement Learning, Diffusion Policy, Maximum Entropy Reinforcement Learning, Reinforcement Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DIME</title>

<!--  &lt;!&ndash; Global site tag (gtag.js) - Google Analytics &ndash;&gt;-->
<!--  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>-->
<!--  <script>-->
<!--    window.dataLayer = window.dataLayer || [];-->

<!--    function gtag() {-->
<!--      dataLayer.push(arguments);-->
<!--    }-->

<!--    gtag('js', new Date());-->

<!--    gtag('config', 'G-PYVRSFMDRL');-->
<!--  </script>-->

    <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->
<!--  <link rel="icon" href="./static/images/Untitled.ico">-->
    <link rel="icon" href="./static/images/alr-logo_large.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DIME: Diffusion-Based Maximum Entropy Reinforcement Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href=https://onur4229.github.io/>Onur Celik</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href=https://supersglzc.github.io target="_blank">Zechu Li</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href=https://denisbless.github.io target="_blank">Denis Blessing</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href=https://brucegeli.github.io target="_blank">Ge Li</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href=https://www.ias.informatik.tu-darmstadt.de/Team/DanielPalenicek target="_blank">Daniel Palenicek</a><sup>3,4</sup>,</span>
            <span class="author-block">
              <a href=https://www.ias.informatik.tu-darmstadt.de/Team/JanPeters target="_blank">Jan Peters</a><sup>3,4,5,6</sup>,</span>
            <span class="author-block">
              <a href=https://pearl-lab.com/people/georgia-chalvatzaki target="_blank">Georgia Chalvatzaki</a><sup>2,4</sup>,</span>
            <span class="author-block">
              <a href=https://alr.iar.kit.edu/21_65.php target=_blank>Gerhard Neumann</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Autonomous Learning Robots (ALR), Karlsruhe Institute of Technology (KIT)</span>
            <span class="author-block"><sup>2</sup>Interactive Robot Perception & Learning (PEARL), TU Darmstadt </span>
            <span class="author-block"><sup>3</sup>Intelligent Autonomous Systems Group (IAS), TU Darmstadt </span>
            <span class="author-block"><sup>4</sup>Hessian.AI  </span>
            <span class="author-block"><sup>5</sup>German Research Center for AI (DFKI)  </span>
            <span class="author-block"><sup>6</sup>Centre for Cognitive Science, TU Darmstadt  </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/forum?id=Aw6dBR7Vxj"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2502.02316"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ALRhub/DIME"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
              This paper was published at <a href="https://icml.cc/Conferences/2025"> ICML 2025 </a>.
              The code can be found in the corresponding github repository together with all learning curves for replotting.
              We have run DIME additionally on all environments of the DMC suite and have put the results alongside the ones from the paper in the github repository.
              Please reach out to us if you have questions or if you are interested in collaborations.
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--      <img src="./static/images/fig1.png">-->
<!--      Figure 1: During inference, the Mixture of Experts (MoE) policy observes a context $\boldsymbol{c}$ and selects an-->
<!--      expert to execute the corresponding skill. During training, the MoE model selects the context $\boldsymbol{c}_T$-->
<!--      it prefers through sampling from the per-expert energy-based context distribution. This preferred context sampling-->
<!--      enables automatic curriculum learning.-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
            Maximum entropy reinforcement learning (MaxEnt-RL) has become the standard approach to RL due to its beneficial exploration properties.
            Traditionally, policies are parameterized using Gaussian distributions, which significantly limits their representational capacity. Diffusion-based
            policies offer a more expressive alternative, yet integrating them into MaxEnt-RL poses challengesâ€”primarily due to the intractability of
            computing their marginal entropy. To overcome this, we propose Diffusion-Based Maximum Entropy RL (DIME). DIME leverages recent
            advances in approximate inference with diffusion models to derive a lower bound on the maximum entropy objective. Additionally, we propose a
            policy iteration scheme that provably converges to the optimal diffusion policy. Our method enables the use of expressive diffusion-based policies
            while retaining the principled exploration benefits of MaxEnt-RL, significantly outperforming other diffusion-based methods on challenging high-dimensional control benchmarks. It is also
            competitive with state-of-the-art non-diffusion based RL methods while requiring fewer algorithmic design choices and smaller update-to-data ratios, reducing computational complexity.
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

  <h2 class="title is-3">Maximum Entropy Reinforcement Learning with Diffusion Policies</h2>
      DIME is a reinforcement learning algorithm that allows training diffusion policies in the maximum entropy RL framework.
      Notably, DIME allows controlling the exploration-exploitation trade-off of diffusion policies by adjusting the entropy-scaling parameter $\alpha$.
      The resulting behavior for the generative process can be seen in the following Figure.
      <img src="./static/images/alpha_effect.png">
      Figure 1: <strong> The effect of the reward scaling parameter $\alpha$ </strong>. The figures in (a)-(b) show
      diffusion processes for different $\alpha$ values starting at a prior distribution $\mathcal{N}(0,I)$ and going
      backward in time to approximate the target distribution $\exp{\left(Q^\pi/\alpha\right)}/Z^\pi$.
      Small values for $\alpha$ (a) lead to concentrated target distributions with less noise in the diffusion
      trajectories especially at the last time steps. The higher $\alpha$ becomes (b) and (c), the more the target
      distribution is smoothed and the distribution of the samples at the last time steps becomes more noisy.
      Therefore, the parameter $\alpha$ directly controls the exploration by enforcing noisier samples the higher $\alpha$ becomes.

    <br>
    <br>
    <br>
    Optimizing the maximum entropy RL objective is difficult because the marginal likelihood is difficult to compute for diffusion policies.
    DIME's key contribution is a lower-bound to this maximum entropy objective that can be evaluated and optimized efficiently.
    The resulting lower-bound is the ratio between the backward and forward diffusion process, where the forward process starts at the target distribution
      $\vec{\pi}_0(a^0|s)=\frac{\exp{Q_\phi(s,a^0)}}{Z_\phi(s)}$, thereby including the Q-function in the policy update loss function.
      Please find the exact equations in the paper.
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
  <h2 class="title is-3"> Entropy Scaling Benefits and Improved Performance over a Gaussian Policy</h2>

    <img src="./static/images/ablation_alpha_gauss_diffusion.png">
    Figure 2: <strong> Entropy Scaling Sensitivity (a)-(b) </strong>. The $\alpha$ parameter controls the
      exploration-exploitation trade-off. (a) shows the learning curves for varying values on DMC's dog-run task.
      Too high $\alpha$ values ($\alpha=0.1$) do not incentivize learning whereas too small $\alpha$ values ($\alpha\leq10^{-5}$) converge to suboptimal behavior.
      (b) shows the aggregated end performance for each learning curve in (a).
      For increasing $\alpha$ values, the end performance increases until it reaches an optimum at $\alpha=10^{-3}$ after which the performance starts dropping.
      <strong> Diffusion Policy Benefit (c) and (d).</strong> We compare DIME to a Gaussian policy with the same implementation details as DIME on the (a) humanoid-run and (b) dog-run tasks.
      The diffusion-based policy reaches a higher return (a) and converges faster.
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
  <h2 class="title is-3"> Number of Diffusion Steps and DIME's performance compared to other Diffusion-Based Baselines</h2>

    <img src="./static/images/diffusion_steps_gym_envs.png">
    Figure 3: <strong> Varying the Number of diffusion steps (a)-(b).</strong> The number of diffusion steps might
      affect the performance and the computation time. (a) shows DIME's learning curves for varying diffusion steps on DMC's humanoid-run task.
      <em>Two</em> diffusion steps perform badly, whereas <em>four</em> and <em>eight</em> diffusion steps perform similar but still worse than <em>16</em> and <em>32</em> diffusion steps which perform similarly.
      (b) shows the computation time for 1MIO steps of the corresponding learning curves.
      The smaller the diffusion steps, the less computation time is required.
      <strong> Learning Curves on Gym Benchmark Suite (c)-(d).</strong>
      We compare DIME against various diffusion baselines and CrossQ on the (c) <em>Ant-v3</em> and (d) <em>Humanoid-v3</em> from the Gym suite.
      While all diffusion-based methods are outperformed by DIME, DIME performs on par with CrossQ on the Ant environment.
      DIME performs favorably on the high-dimensional <em>Humanoid-v3</em> environment, where it also outperforms CrossQ.
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
  <h2 class="title is-3"> Comparison to SOTA RL Methods on DMC and MYO Suite</h2>
  DIME is compared against most recent SOTA RL methods on the humanoid and dog environments from DMC and the difficult environments from the MYO Suite.
  DIME performs favorably on the high-dimensional dog tasks and performs on par against the SOTA Gaussian policy-based method BRO.
<br>
      <br>
    <img src="./static/images/dmc_myo.png">
    Figure 4: <strong> Training curves on DMC's dog, humanoid tasks, and the hand environments from the MYO Suite.</strong>
      DIME performs favorably on the high-dimensional dog tasks, where it significantly outperforms all baselines (dog-run)
      or converges faster to the final performance.
      On the humanoid tasks, DIME outperforms all diffusion-based baselines, CrossQ and BRO Fast, and performs on par
      with BRO on the humanoid-stand task and slightly worse on the humanoid-run and humanoid-walk tasks.
      In the MYO SUITE environments, DIME performs consistently on all tasks, either outperforming the baselines or performing on par.
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{
celik2025dime,
title={{DIME}: Diffusion-Based Maximum Entropy Reinforcement Learning},
author={Onur Celik and Zechu Li and Denis Blessing and Ge Li and Daniel Palenicek and Jan Peters and Georgia Chalvatzaki and Gerhard Neumann},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=Aw6dBR7Vxj}
}
</code></pre>
  </div>
</section>


<!--<footer class="footer">-->
<!--  <div class="container">-->
<!--    <div class="content has-text-centered">-->
<!--      <a class="icon-link"-->
<!--         href="./static/videos/nerfies_paper.pdf">-->
<!--        <i class="fas fa-file-pdf"></i>-->
<!--      </a>-->
<!--      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>-->
<!--        <i class="fab fa-github"></i>-->
<!--      </a>-->
<!--    </div>-->
<!--    <div class="columns is-centered">-->
<!--      <div class="column is-8">-->
<!--        <div class="content">-->
<!--          <p>-->
<!--            This website is licensed under a <a rel="license"-->
<!--                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative-->
<!--            Commons Attribution-ShareAlike 4.0 International License</a>.-->
<!--          </p>-->
<!--          <p>-->
<!--            This means you are free to borrow the <a-->
<!--              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,-->
<!--            we just ask that you link back to this page in the footer.-->
<!--            Please remember to remove the analytics code included in the header of the website which-->
<!--            you do not want on your website.-->
<!--          </p>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</footer>-->

</body>
</html>
